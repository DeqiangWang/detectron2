import math
import torch
import numpy as np
from torch import nn
import fvcore.nn.weight_init as weight_init
import torch.nn.functional as F

from detectron2.layers import (
    ShapeSpec,
    get_norm,
)

from detectron2.modeling.backbone.resnet import ResNetBlockBase

from detectron2.modeling import Backbone
from detectron2.modeling import BACKBONE_REGISTRY
from projects.Backbone.nets.darknet53 import Conv2dBNLeakyReLU
# from .darknet import build_darknet_backbone
from projects.Backbone.nets.darknet53 import build_darknet_backbone

class YOLOV3FPNFeatureExtractor(nn.Module):
    """
    Top down means direction from output to input.
    Bottom up means direction from input to output.
    """
    def __init__(self,
                 in_channels,
                 out_channels,
                 up_sampling=False,
                 norm="BN"):
        super(YOLOV3FPNFeatureExtractor, self).__init__()
        # in_channels: 1024 out_channels: 512
        # in_channels: 512 out_channels: 256
        # in_channels: 256 out_channels: 128
        first_in_channels = in_channels
        if up_sampling:
            self.top_feature = nn.Sequential(
                Conv2dBNLeakyReLU(in_channels, out_channels,
                                  kernel_size=1, stride=1, padding=0, norm=norm),
                nn.UpsamplingNearest2d(scale_factor=2)
            )
            first_in_channels = in_channels + out_channels
        self.head_feature = nn.Sequential(
            Conv2dBNLeakyReLU(first_in_channels, out_channels,
                              kernel_size=1, stride=1, padding=0, norm=norm),
            Conv2dBNLeakyReLU(out_channels, out_channels * 2,
                              kernel_size=3, stride=1, padding=1, norm=norm),
            Conv2dBNLeakyReLU(out_channels * 2, out_channels,
                              kernel_size=1, stride=1, padding=0, norm=norm),
            Conv2dBNLeakyReLU(out_channels, out_channels * 2,
                              kernel_size=3, stride=1, padding=1, norm=norm),
            Conv2dBNLeakyReLU(out_channels * 2, out_channels,
                              kernel_size=1, stride=1, padding=0, norm=norm),
        )

    def forward(self, x, top_down_features=None):
        if top_down_features is not None:
            
            up_sampling = self.top_feature(top_down_features)
            x = torch.cat((up_sampling, x), dim=1)
            
        x = self.head_feature(x)

        return x

class DarknetFPN(Backbone):
    """
    This module implements Feature Pyramid Network.
    It creates pyramid features built on top of some input feature maps.
    """

    def __init__(
        self, bottom_up, in_features, out_channels, norm="BN"
    ):
        """
        Args:
            bottom_up (Backbone): module representing the bottom up subnetwork.
                Must be a subclass of :class:`Backbone`. The multi-scale feature
                maps generated by the bottom up network, and listed in `in_features`,
                are used to generate FPN levels.
            in_features (list[str]): names of the input feature maps coming
                from the backbone to which FPN is attached. For example, if the
                backbone produces ["s3", "s4", "s5"], any *contiguous* sublist
                of these may be used; order must be from high to low resolution.
            out_channels (list[int]): number of channels in the output feature maps.
            norm (str): the normalization to use.
        """
        super(DarknetFPN, self).__init__()
        assert isinstance(bottom_up, Backbone)
        assert len(in_features) == len(out_channels), "lengh of output channels should be equal to length of in features."

        # Feature map strides and channels from the bottom up network (e.g. ResNet)
        # in_strides = [bottom_up.out_feature_strides[f] for f in in_features]
        # in_channels = [bottom_up.out_feature_channels[f] for f in in_features]
        input_shapes = bottom_up.output_shape()
        in_strides = [input_shapes[f].stride for f in in_features]
        in_channels = [input_shapes[f].channels for f in in_features]

        _assert_strides_are_log2_contiguous(in_strides)

        extra_feature_extractor = []

        for idx, (in_channels, out_channel) in enumerate(zip(in_channels, out_channels)):
            feature_extractor = YOLOV3FPNFeatureExtractor(
                in_channels,
                out_channel,
                up_sampling=(idx < len(in_features) - 1),
                norm=norm)
            # print(idx < len(in_features) - 1)
            stage = int(math.log2(in_strides[idx]))
            self.add_module("fpn_feature_s{}".format(stage), feature_extractor)
            extra_feature_extractor.append(feature_extractor)
        # Place convs into top-down order (from low to high resolution)
        # to make the top-down computation in forward clearer.
        self.extra_feature_extractor = extra_feature_extractor[::-1]
        self.in_features = in_features
        self.bottom_up = bottom_up
        # Return feature names are "p<stage>", like ["p2", "p3", ..., "p6"]
        self._out_feature_strides = {"p{}".format(
            int(math.log2(s))): s for s in in_strides}

        self._out_features = list(self._out_feature_strides.keys())

        self._out_feature_channels = {
            k: v for k, v in zip(self._out_features, out_channels)
        }
        self._size_divisibility = in_strides[-1]

    @property
    def size_divisibility(self):
        return self._size_divisibility

    def forward(self, x):
        """
        Args:
            x (dict[str: Tensor]): mapping feature map name (e.g., "s5") to
                feature map tensor for each feature level in high to low resolution order.

        Returns:
            dict[str: Tensor]:
                mapping from feature map name to FPN feature map tensor
                in high to low resolution order. Returned feature names follow the FPN
                paper convention: "p<stage>", where stage has stride = 2 ** stage e.g.,
                ["p2", "p3", ..., "p5"].
        """
        # Reverse feature maps into top-down order (from low to high resolution)
        bottom_up_features = self.bottom_up(x)
        x = [bottom_up_features[f] for f in self.in_features[::-1]]
        results = []
        top_down_features = self.extra_feature_extractor[0](x[0])
        results.append(top_down_features)
        for features, extra_feature_extractor in zip(
            x[1:], self.extra_feature_extractor[1:]
        ):
            top_down_features = extra_feature_extractor(features, top_down_features=top_down_features)
            results.insert(0, top_down_features)

        assert len(self._out_features) == len(results)
        return dict(zip(self._out_features, results))

    def output_shape(self):
        return {
            name: ShapeSpec(
                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]
            )
            for name in self._out_features
        }
    
    def get_conv_bn_modules(self):
        """
        for weight convert from original yolo weights file
        order is from top to down
        """
        modules = []
        for f in self.extra_feature_extractor:
            modules_i = []
            for name, module in f.named_modules():
                if isinstance(module, (nn.Conv2d, nn.BatchNorm2d)):
                    modules_i.append(module)
            modules.append(modules_i)
        return self.bottom_up.get_conv_bn_modules(), modules


def _assert_strides_are_log2_contiguous(strides):
    """
    Assert that each stride is 2x times its preceding stride, i.e. "contiguous in log2".
    """
    for i, stride in enumerate(strides[1:], 1):
        assert stride == 2 * strides[i - 1], "Strides {} {} are not log2 contiguous".format(
            stride, strides[i - 1]
        )


@BACKBONE_REGISTRY.register()
def build_darknet_fpn_backbone(cfg, input_shape: ShapeSpec):
    """
    Args:
        cfg: a detectron2 CfgNode

    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    bottom_up = build_darknet_backbone(cfg, input_shape)
    in_features = cfg.MODEL.DarknetFPN.IN_FEATURES  # ["s3", "s4", "s5"]
    out_channels = cfg.MODEL.DarknetFPN.OUT_CHANNELS  # [128, 256, 512]
    backbone = DarknetFPN(
        bottom_up=bottom_up,
        in_features=in_features,
        out_channels=out_channels,
    )
    return backbone
